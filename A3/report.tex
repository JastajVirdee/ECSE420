\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{listings}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{float}
\usepackage{color}
\usepackage{courier}
\usepackage{dirtytalk}
\usepackage{hyperref}
\usepackage{fixltx2e}
\usepackage[margin=1in]{geometry}

\definecolor{dblue}{rgb}{0.2,0.2,0.6}
\definecolor{dyellow}{rgb}{0.6,0.6,0.2}

\lstset{
	basicstyle=\footnotesize\ttfamily,
	breaklines=true,
	showstringspaces=false,
	frame=single,
	showlines=true,
	numbers=left,
}

\lstdefinelanguage{bash}{
	morestring=[b]"",
	morestring=[s]{>}{<},
	stringstyle=\color{black},
	identifierstyle=\color{blue},
	keywordstyle=\color{green},
}

\hypersetup {
	colorlinks = true,
	linkcolor = dblue,
	linktoc = all,
	citecolor = dyellow,
}

\title{Assignment 3}
\author{Spiros-Daniel Mavroidakos - 260689391\\Jastaj Virdee - 260689027}
\date{\today}

\begin{document}
\pagenumbering{gobble}
\maketitle
\pagenumbering{roman}
\newpage
\tableofcontents
\newpage
\pagenumbering{arabic}

\section{Question 1}

\subsection{}
From the question, we know the following.
\begin{itemize}
	\item We are reading L words from cache
	\item Cache Line is 4 words
\end{itemize}
L' is the number of words that can be stored in the cache given a maximum word seperation of stride s. A stride can be a maximum size of 
$\left\lfloor\dfrac{L}{2}\right\rfloor$. We also know that the cache can have several cache lines. We will denote the amount of cache lines 
by k. \\
Knowing this, we can come up with the following equation for L'.

\begin{equation}
	\begin{split}
		L' = \left\lceil\frac{4k}{\left\lfloor\dfrac{L}{2}\right\rfloor}\right\rceil
	\end{split}
\end{equation}

Furthermore, t0 shows the cache access latency as it does take some time to read from the cache. GIven that L < L', the stride size will 
either be 0 or a size that fits within the case. Therefore, all the time delay for access comes from reading the case.

\subsection{}
Given that L $>$ L', the stride will be large enough that we cannot only read from the cache. The words will be too seperated to read only 
from cache. However, at t1, we have peaked in the maximum read time. This implies that the stride is larger than the actual cache size. 
That is why it is a constant line. t1 will therefore be showing the memory access latency since we are no longer reading from the cache. 
t1 might also take into account the time to read the cache as we need to generate a cache miss to read from other storage.

\subsection{}
Part 1 is when L $<$ L' so as stated in section 1.1, all of the data of the array will fit into the cache and will therefore be accessed at 
the time that it takes to read the cache. This is why it is the lowest of the 2 curves. \\
Part 2 is when L $>$ L' however, the stride has not become so big that we cannot use the cache in a meaningful way. Throughout part 2, the 
stride is continuously increasing which is why the curve is increasing as you must read from cache and then main memory. At the end of 
part 2 (the peak value where it becomes constant) is where part 3 begins and when the stride has become so large that the stride takes 
up all of the cache and you will always incur a cache miss and have to then go to main memory which is why it is a peak. Part 2 has a mix 
between some values are in cache and some are not while part 3 is where everything is not in the cache. 

\subsection{}
If we were to pad the array such that distinct elements are mapped to distinct cache lines then the value of L' will surely be affected.
From 1.1, we know that we have k cache lines and that every cache line can contain 4 words. However, by padding we reduce the amount of 
words that a cache line can hold. It can now only hold 1 word. Therefore, we would get the following revised equation:
\begin{equation}
	\begin{split}
		L' = \left\lceil\frac{k}{\left\lfloor\dfrac{L}{2}\right\rfloor}\right\rceil
	\end{split}
\end{equation}
Because of this, performance degredation will happen since the value of L' will be reduced. A smaller L' means that we will have more cache 
misses as we can store less words in the cache and therefore will have to read from main memory more often which is noticeably more costly 
than reading from a cache.

\section{Question 2}

\subsection{}
All of the source code can be found in the appendix in the specified sections:
\begin{itemize}
	\item The implementation for a node can be found in section A.1. 
	\item The contains method implementation can be found in section A.2 
\end{itemize}

\subsection{}
All of the source code can be found in the appendix in the specified sections:
\begin{itemize}
	\item The implementation for the contains Test can be found in section A.3
	\item The output of the test can be found in Section A.4.
\end{itemize}

The implementation of the contains method was doen using a hand-over-hand locking style. This assured that you would lock the previous node 
and the current node. This is needed to ensure that when you are selecting the next node, that you do not get preempted by another operation 
that came after you. For example, if you only lock the current node, then it is possible that you unlock the current node and then you set your 
next node but you have not locked this node. Now, the thread gets suspended and another one comes along and removes the node you currently have 
selected. You resume and now you are on an orphan node. This is not safe and is why you need to lock both the previous and current. You release 
the previous and then lock the current node's next node. Like this, the current node blocks anyone from overtaking you.
\\\\
The test implemented here if several threads tried to check for the existance of the same node. From the output, you can clearly see that 
hand-over-hand locking is occuring due to the order of locks and release. Furthermore, the locks are working as expected since no lock 
is being locked without an unlock occuring prior. The output clearly demonstrates a previous and current node being locked. Then the 
previous node is unlocked and the current node's next node is locked. It can also be noticed that the released previous node is then 
being locked by another waiting node. Since the add and remove methods that have been seen in class are implemented using the same 
hand-over-hand method, those operations cannot interfere with the contains method as they are designed to be thread safe.
\\\\
The test setup is relatively simple, 3 nodes are set up each with a different value: Node a has a value of 1, node b has a value of 2
and node c has a value of 3. The head node's next value is set up to be node a. Node a points to b, b points to c and finally c points 
to the tail node which marks the end of the list. From the source code, the contains method will return false only if we reach the tail 
node as all other nodes have been checked for equality. For simplicity, each node is going to be searching for the value of 3. From the 
output, we can observe that each thread will find the value at a different time and not all at the same time. If the output would show 
lock duplication before an unlock or at least 2 threads finding the value conccurently then we would know that the implementation is 
flawed. However, this is not the case and therefore, the test passes. 

\section{Question 3}

\subsection{}
The contents of the (circular) array-based queue will be very similar to the one shown in class (which used a linked-list). Notable differences 
would be that the head and tail will be represented by integers, representing the index of the array. Both will be initialized to zero. 
The array will also need to be created using the `int capacity` variable to set the size.\\
The equeue method will also be very similar in that both implementations will lock the `enqLock` before attempting to enqueue an object, 
and will release it once it is finished. It will also wait while the queue is full and notify blocked dequeuers if the queue was empty. 
The difference comes when one attempts to enqueue an item. Rather than creating a new node, the item will be placed in the array at index 
`(tail+1)\%capacity`,`tail` and `size` will then be incremented.\\
Similarly, the dequeue method will lock the `deqLock` before attempting to dequeue an object, and release it once it is finished. It will 
then wait while the queue is empty. To dequeue an object, the item at the `head\%capacity` index of the array will be saved, and later 
returned, `head` will be incremented and `size` will be decremented. Finally the method will notify the blocked enqueuers if the queue 
was full.

\subsection{}
To perform a lock-free enqueue, one would have to perform a logical enqueue enqueue by performing a Compare and Set between the item at 
the `tail` index and the new item. One would then have to perform a physical enqueue by performing a CAS between the `tail` index and the 
index of the new item. However these two steps are not atomic, so the `tail` index may either point to the actual last item or the 
penultimate item. For the original implementation using linked-lists, if one were to find a trailing tail one would fix it by checking if 
the tail node has a non-null next field, and then performing a CAS on the tail and tail.next field. This is where a problem arises when 
trying to use arrays because checking if the item at the index after `tail` is not null does not not guarantee we have a  trailing tail. 
Because we are using a circular queue, it is possible that the `tail` index is in fact pointing to the correct index, but the next index 
may be non-null.\\
To solve this, one would have to switch to a linear, fixed size array. Enqueuing would consist of placing the item at the first index 
which has a null field. Dequeuing would consist of saving and later returning the item in the first index, and shifting all the other 
items to the previous position.

\section{Question 4}

\subsection{}
The algorithm for sequential matrix vector multiplication involves computing each entry of the result vector by performing the dot 
product on the rows of the matrix with the vector. The code can be found in section B.1.

\subsection{}
The algorithm used of parallel matrix vector multiplication involved splitting up the problem into multiple subtasks. Specifically, each 
subtask was responsible for calculating one entry of the result vector. To do this it, the dot product was performed on one row of the 
matrix and the vector. The work and critical path will be discussed in part 4. The code can be found in Appendix B.2.

\subsection{}
To test the execution times of the sequential and parallel methods, a 2000 by 2000 randomly generated matrix was created as well as a 
randomly generated 2000-length vector. The time was recorded and the sequential method was executed. Once finished, the time was 
recorded again and the difference between the two times was calculated to determine the time of execution. The same was done for the 
parallel method, where 3 threads were used. The execution time of the sequential method was 0.142s, the parallel time was 0.109s. The 
speedup on P processors is the time taken by one processor divided the time taken by P processors. In this example it would be the time 
of the sequential method divided by the time of the parallel method. Hence the speed up on 3 processors is 1.30. The code can be found 
in section B.3.

\subsection{}
Each subtask performs n multiplications and n-1 additions. There are n subtasks so the work is $n*(n+n-1)$ which is $O(n^2)$. Since all 
the critical paths can be executed in parallel, the critical path would be the cost of one subtask, so $n+n-1$ which is $O(n)$. The 
parallelism is the work divided by the critical path: $O(n^2/n) = O(n)$.

\appendix
\section{Question 2 Code}
\subsection{Node Implementation}
\lstinputlisting[language=java]{Node.java}
\subsection{Contains Implementation}
\lstinputlisting[language=java]{FineGrainedContains.java}
\subsection{Contains Test}
\lstinputlisting[language=java]{ContainsTest.java}
\subsection{Contains Test Output}
\lstinputlisting[language=bash]{output.txt}

\section{Question 4 Code}
\subsection{Sequential Implementation}
\lstinputlisting[language=java]{SeqMatrixVectorMult.java}
\subsection{Parallel Implementation}
\lstinputlisting[language=java]{ParMatrixVectorMult.java}
\subsection{Test Program}
\lstinputlisting[language=java]{MatrixVectorMultTest.java}

\end{document}
